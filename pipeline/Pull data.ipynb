{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb34c3f6-0846-48ab-b3ea-28026f58ac27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets to receive job parameters\n",
    "dbutils.widgets.text(\"catalog\", \"mk_fiddles\")\n",
    "dbutils.widgets.text(\"schema\", \"detroit_911\")\n",
    "\n",
    "# Get parameter values\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name = \"incidents_bronze\"\n",
    "table = f\"{catalog}.{schema}.{table_name}\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceb172d-410c-413d-8f7f-de5b7e92fe9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "TODAY = dt.datetime.now()\n",
    "RAW_CALLS_ROUTE: str = (\n",
    "    \"https://services2.arcgis.com/qvkbeam7Wirps6zC/arcgis/rest/services/Police_Serviced_911_Calls/FeatureServer/0/query?where=called_at>'{}'&outFields=*&returnGeometry=false&f=json\"\n",
    ")\n",
    "\n",
    "def _get_latest_date_in_dataset(table: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the latest date in the dataset minus one day.\n",
    "\n",
    "    Determines the start date for the API call\n",
    "    \"\"\"\n",
    "    df = spark.sql(f\"SELECT max(called_at) as latest_call_date FROM {table}\")\n",
    "    latest_datetime = pd.to_datetime(df.first()[\"latest_call_date\"], unit='ms') - dt.timedelta(days=1)\n",
    "    return latest_datetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def pull_crime_data(num_records: int, latest_call_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull data from the Detroit Police Department's 911 Call for Service API.\n",
    "    API call generated here: https://data.detroitmi.gov/datasets/detroitmi::911-calls-for-service-last-30-days/about\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.DataFrame()\n",
    "    for i, offset in enumerate(range(0, num_records + 2000, 2000)):\n",
    "        data = requests.get(RAW_CALLS_ROUTE.format(latest_call_date) + f\"&resultOffset={offset}\")\n",
    "        data = data.json()\n",
    "        records = [row[\"attributes\"] for row in data.get(\"features\")]\n",
    "        df = pd.DataFrame(records)\n",
    "        start_shape = raw_data.shape\n",
    "        raw_data = pd.concat((raw_data, df), axis=0)\n",
    "        if raw_data.drop_duplicates().shape == start_shape:\n",
    "            print(\"DataFrame is the same size as before. Exiting loop...\")\n",
    "            break\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Data Pulled for {offset} records\")\n",
    "            # be nice or pay the price\n",
    "            time.sleep(1)\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        latest_date = _get_latest_date_in_dataset(table)\n",
    "        existing_records = spark.sql(f\"SELECT incident_id from {table}\")\n",
    "    except Exception as e:\n",
    "        latest_date = (dt.datetime.now() - dt.timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
    "        existing_records = spark.createDataFrame(data=pd.DataFrame({\"incident_id\": [0]}))\n",
    "\n",
    "    count_url = RAW_CALLS_ROUTE.format(latest_date) + \"&returnCountOnly=true\"\n",
    "    record_count = requests.get(count_url)\n",
    "    num_records = int(record_count.json()[\"count\"])\n",
    "    print(f\"Total Records since last pull: {num_records}\")\n",
    "\n",
    "    detroit_crime_raw = pull_crime_data(num_records, latest_date)\n",
    "    detroit_crime_raw[\"day\"] = (dt.datetime.now() + pd.DateOffset(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    detroit_crime_raw = spark.createDataFrame(detroit_crime_raw)\n",
    "    detroit_crime_raw = detroit_crime_raw.join(existing_records, on=\"incident_id\", how=\"leftanti\")\n",
    "\n",
    "    return detroit_crime_raw\n",
    "\n",
    "detroit_crime_raw = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b032539-924f-4c40-96f8-04348873fdda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if detroit_crime_raw.count() > 0:\n",
    "  print(f\"Writing {detroit_crime_raw.count()} records to {table}\")\n",
    "  detroit_crime_raw.write.mode(\"append\").saveAsTable(f\"mk_fiddles.detroit_911.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5415309452500388,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Pull data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
